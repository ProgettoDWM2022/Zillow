{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELPU3bTvsRXC"
   },
   "source": [
    "# Predictions \n",
    "## Niccolò Simonato \n",
    "## Data & Web Mining, Academic Year 2021-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dependencies and the cleaned dataset\n",
    "\n",
    "The cleaned dataset is now imported.\n",
    "\n",
    "The first snipped is intended to be used in the Google Drive environment, just set the path variable as needed.\n",
    "\n",
    "The second one is intended to be used in the Jupyter Notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from math import floor, log10\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_x2HBMo0iWLY"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "# path = '/gdrive/MyDrive/Progetto DWM/Data/*.csv'\n",
    "# %cd /gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OlRC0QJQiT25"
   },
   "outputs": [],
   "source": [
    "path = 'Data/'\n",
    "\n",
    "# cleaned_df = pd.read_csv(path, low_memory = False)\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for i in range(4):\n",
    "    train_datasets.append(pd.read_csv(f\"{path}train_dataset_2016_{i + 1}.csv\", low_memory = True))\n",
    "    test_datasets.append(pd.read_csv(f\"{path}test_dataset_2016_{i + 1}.csv\", low_memory = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "In order to prevent any exception to be raised, we will check the state of each dataset and will ensure that the data can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>parcelid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>lotsizesquarefeet</th>\n",
       "      <th>...</th>\n",
       "      <th>assessmentyear</th>\n",
       "      <th>landtaxvaluedollarcnt</th>\n",
       "      <th>taxamount</th>\n",
       "      <th>logerror</th>\n",
       "      <th>N-LivingAreaError</th>\n",
       "      <th>N-LivingAreaProp</th>\n",
       "      <th>N-ValueRatio</th>\n",
       "      <th>N-LogTaxScore</th>\n",
       "      <th>DD</th>\n",
       "      <th>N-DaysInCurrentYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63945.000000</td>\n",
       "      <td>6.394500e+04</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>63945.0</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.0</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "      <td>63945.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48882.100227</td>\n",
       "      <td>1.300342e+07</td>\n",
       "      <td>2.272383</td>\n",
       "      <td>3.005145</td>\n",
       "      <td>5.520320</td>\n",
       "      <td>1744.500774</td>\n",
       "      <td>1744.500774</td>\n",
       "      <td>45.123354</td>\n",
       "      <td>66.106881</td>\n",
       "      <td>3.976152</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>5.208905</td>\n",
       "      <td>5961.794141</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.251780</td>\n",
       "      <td>76.103164</td>\n",
       "      <td>9.148952</td>\n",
       "      <td>16.356807</td>\n",
       "      <td>164.286027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28905.336939</td>\n",
       "      <td>2.423721e+06</td>\n",
       "      <td>0.941331</td>\n",
       "      <td>1.010511</td>\n",
       "      <td>1.531799</td>\n",
       "      <td>904.481611</td>\n",
       "      <td>904.481611</td>\n",
       "      <td>18.146744</td>\n",
       "      <td>19.240454</td>\n",
       "      <td>0.471434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478561</td>\n",
       "      <td>6908.052778</td>\n",
       "      <td>0.155913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.208977</td>\n",
       "      <td>32.475617</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>9.002009</td>\n",
       "      <td>85.762695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.071174e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.222716</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2.973128</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>-3.194000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>2.548288</td>\n",
       "      <td>4.824880</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16874.000000</td>\n",
       "      <td>1.154290e+07</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>31.704379</td>\n",
       "      <td>54.464561</td>\n",
       "      <td>3.740521</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>4.912222</td>\n",
       "      <td>2868.820000</td>\n",
       "      <td>-0.024300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>70.579142</td>\n",
       "      <td>8.756067</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>97.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>56386.000000</td>\n",
       "      <td>1.258800e+07</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.565407</td>\n",
       "      <td>1516.000000</td>\n",
       "      <td>1516.000000</td>\n",
       "      <td>46.238134</td>\n",
       "      <td>67.877974</td>\n",
       "      <td>3.850340</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>5.285692</td>\n",
       "      <td>4534.620000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214267</td>\n",
       "      <td>79.546204</td>\n",
       "      <td>9.182151</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>167.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73348.000000</td>\n",
       "      <td>1.425320e+07</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2059.000000</td>\n",
       "      <td>2059.000000</td>\n",
       "      <td>56.600059</td>\n",
       "      <td>80.998193</td>\n",
       "      <td>4.056600</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>5.538953</td>\n",
       "      <td>6865.160000</td>\n",
       "      <td>0.038300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>84.462565</td>\n",
       "      <td>9.560981</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90272.000000</td>\n",
       "      <td>1.629608e+08</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>20013.000000</td>\n",
       "      <td>20013.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>6.843296</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>7.389166</td>\n",
       "      <td>321936.090000</td>\n",
       "      <td>4.737000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.497006</td>\n",
       "      <td>4997.622067</td>\n",
       "      <td>12.951033</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>365.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0      parcelid   bathroomcnt    bedroomcnt  \\\n",
       "count  63945.000000  6.394500e+04  63945.000000  63945.000000   \n",
       "mean   48882.100227  1.300342e+07      2.272383      3.005145   \n",
       "std    28905.336939  2.423721e+06      0.941331      1.010511   \n",
       "min        0.000000  1.071174e+07      0.000000      0.000000   \n",
       "25%    16874.000000  1.154290e+07      2.000000      2.000000   \n",
       "50%    56386.000000  1.258800e+07      2.000000      3.000000   \n",
       "75%    73348.000000  1.425320e+07      3.000000      4.000000   \n",
       "max    90272.000000  1.629608e+08     20.000000     10.000000   \n",
       "\n",
       "       buildingqualitytypeid  calculatedfinishedsquarefeet  \\\n",
       "count           63945.000000                  63945.000000   \n",
       "mean                5.520320                   1744.500774   \n",
       "std                 1.531799                    904.481611   \n",
       "min                 1.000000                      2.000000   \n",
       "25%                 4.000000                   1172.000000   \n",
       "50%                 5.565407                   1516.000000   \n",
       "75%                 7.000000                   2059.000000   \n",
       "max                12.000000                  20013.000000   \n",
       "\n",
       "       finishedsquarefeet12      latitude     longitude  lotsizesquarefeet  \\\n",
       "count          63945.000000  63945.000000  63945.000000       63945.000000   \n",
       "mean            1744.500774     45.123354     66.106881           3.976152   \n",
       "std              904.481611     18.146744     19.240454           0.471434   \n",
       "min                2.000000      0.000000      0.000000           2.222716   \n",
       "25%             1172.000000     31.704379     54.464561           3.740521   \n",
       "50%             1516.000000     46.238134     67.877974           3.850340   \n",
       "75%             2059.000000     56.600059     80.998193           4.056600   \n",
       "max            20013.000000    100.000000    100.000000           6.843296   \n",
       "\n",
       "       ...  assessmentyear  landtaxvaluedollarcnt      taxamount  \\\n",
       "count  ...         63945.0           63945.000000   63945.000000   \n",
       "mean   ...          2015.0               5.208905    5961.794141   \n",
       "std    ...             0.0               0.478561    6908.052778   \n",
       "min    ...          2015.0               2.973128      64.000000   \n",
       "25%    ...          2015.0               4.912222    2868.820000   \n",
       "50%    ...          2015.0               5.285692    4534.620000   \n",
       "75%    ...          2015.0               5.538953    6865.160000   \n",
       "max    ...          2015.0               7.389166  321936.090000   \n",
       "\n",
       "           logerror  N-LivingAreaError  N-LivingAreaProp  N-ValueRatio  \\\n",
       "count  63945.000000            63945.0      63945.000000  63945.000000   \n",
       "mean       0.012324                1.0          0.251780     76.103164   \n",
       "std        0.155913                0.0          0.208977     32.475617   \n",
       "min       -3.194000                1.0          0.000179      2.548288   \n",
       "25%       -0.024300                1.0          0.142578     70.579142   \n",
       "50%        0.006000                1.0          0.214267     79.546204   \n",
       "75%        0.038300                1.0          0.311500     84.462565   \n",
       "max        4.737000                1.0         11.497006   4997.622067   \n",
       "\n",
       "       N-LogTaxScore            DD  N-DaysInCurrentYear  \n",
       "count   63945.000000  63945.000000         63945.000000  \n",
       "mean        9.148952     16.356807           164.286027  \n",
       "std         0.682353      9.002009            85.762695  \n",
       "min         4.824880      1.000000             1.000000  \n",
       "25%         8.756067      8.000000            97.000000  \n",
       "50%         9.182151     16.000000           167.000000  \n",
       "75%         9.560981     24.000000           232.000000  \n",
       "max        12.951033     31.000000           365.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets[1].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have any real use for it, we can drop the `parcelid` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_datasets:\n",
    "    i.drop(['parcelid'], axis=1, inplace=True)\n",
    "\n",
    "for i in test_datasets:\n",
    "    i.drop(['parcelid'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ar8xvJ5kbrgl"
   },
   "source": [
    "## Predictions - Attempt 1 - k-NN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TdV2Wm4b8jx"
   },
   "source": [
    "### Why k-NN? - Introduction \n",
    "I chose the k-NN algorithm because, usually, the house construction doesn't happen randomly. It's really unusual that a private party builds his own house, with his own money, and wherever he likes: it's more likely that the municipality's dedicated office decides where and how the houses of a given zone are going to be buildt. \n",
    "\n",
    "Therefore, i think is safe to assume that houses of a given zone will have similar prices. The k-NN hopefully will help achiving this target. \n",
    "\n",
    "This attempt will use the [ScikitLearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor) of the k-NN algorithm for prediction.\n",
    "\n",
    "The first attempt will be conducted with the parameter `weights` set as \"uniform\", the second one will use the value \"distance\".\n",
    "\n",
    "The model will be tested with a number of neighbors beetween 6 and 8, because usually these are the value that yield the best results.\n",
    "\n",
    "The following snippet contains the functions that wraps the described procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7FkaJft9tOR2"
   },
   "outputs": [],
   "source": [
    "n_neighbors = [3,4,5,6,7,8]\n",
    "    \n",
    "def train_test_kNN(x_train, y_train, x_test, y_test, n_neighbors, w='distance'):\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=w)\n",
    "    model = knn.fit(x_train, y_train)\n",
    "    prediction = model.predict(x_test)\n",
    "    scores = model.score(x_test, y_test)\n",
    "    data = {'n_neighbors': n_neighbors, \n",
    "            'weights': w,\n",
    "            'prediction': prediction,\n",
    "            'score' : scores,\n",
    "            'MSE': mean_squared_error(y_test,prediction)\n",
    "           }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset 0 at 2022-06-10 17:43:09.199022: n_neighbor = 3, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:43:37.014510. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:43:37.014510: n_neighbor = 3, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:44:08.533146. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:44:08.533146: n_neighbor = 4, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:44:45.115288. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:44:45.116009: n_neighbor = 4, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:45:21.444762. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:45:21.444762: n_neighbor = 5, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:45:59.426635. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:45:59.426635: n_neighbor = 5, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:46:34.589134. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:46:34.589134: n_neighbor = 6, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:47:09.957454. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:47:09.957454: n_neighbor = 6, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:47:46.379120. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:47:46.379120: n_neighbor = 7, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:48:21.952676. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:48:21.952676: n_neighbor = 7, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:48:58.296921. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:48:58.296921: n_neighbor = 8, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:49:35.632476. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:49:35.632476: n_neighbor = 8, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:50:11.400030. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:50:11.400030: n_neighbor = 9, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:50:48.304822. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:50:48.304822: n_neighbor = 9, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:51:25.087128. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:51:25.087128: n_neighbor = 10, weight = distance\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:52:02.079141. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:52:02.080137: n_neighbor = 10, weight = uniform\n",
      "Experiment number 0 ended succesfully at 2022-06-10 17:52:39.803010. ETA: 72 minutes\n",
      "Using dataset 0 at 2022-06-10 17:52:39.803010: n_neighbor = 11, weight = distance\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "types = ['distance', 'uniform']\n",
    "to_Y = ['logerror']\n",
    "to_X = list(set(train.columns) - set('logerror'))\n",
    "train, test = None, None\n",
    "for i in range(len(train_datasets)):\n",
    "    train = train_datasets[i]\n",
    "    test  = test_datasets[i]\n",
    "    for n in n_neighbors:\n",
    "        for w in types:\n",
    "            print(f\"Using dataset {i + 1} at {datetime.now()}: n_neighbor = {n}, weight = {w}\")\n",
    "            results.append(train_test_kNN(train[to_X], train[to_Y], test[to_X], test[to_Y], n, w))      \n",
    "            print(f\"Experiment ended succesfully at {datetime.now()}. \" + \n",
    "                  f\"ETA: {(len(train_datasets) - i)*(len(n_neighbors) - (n - 2))} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPlmoW8Bd0UP"
   },
   "source": [
    "### How did it go? - Evaluation\n",
    "After obtaining the results, we can proceed with the evaluation of the results.\n",
    "\n",
    "In order to keep this notebook as clean as possible, the evaluation will be done with the built-in evaluator of the KNeighborsRegressor object. The built-in functions uses the $R^2$ index, the coefficient of determination that is $1 - \\frac{u}{v}$, where $u$ is the residual sum of squares and $v$ is the total sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dkMbdppjvYVu"
   },
   "outputs": [],
   "source": [
    "def show_results_kNN(data):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    for parameters in data:\n",
    "        if parameters['weights'] == \"uniform\":\n",
    "            ax1.scatter(parameters['n_neighbors'], parameters['score'], color='darkorange', label='data')\n",
    "            ax1.get_yaxis().get_major_formatter().set_useOffset(False)\n",
    "        else:\n",
    "            ax2.scatter(parameters['n_neighbors'], parameters['score'], color='blue', label='data')\n",
    "            ax2.get_yaxis().get_major_formatter().set_useOffset(False)\n",
    "    plt.xticks(ticks=n_neighbors)\n",
    "    plt.xlabel(\"Number of neighbors\")\n",
    "    plt.title(f\"KNeighborsRegressor\")   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results_kNN(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basing on these results, it seems like the best results are yielded by using the weights parameter set as ‘distance’ (it weights points by the inverse of their distance, so closer neighbors of a query point will have a greater influence).\n",
    "\n",
    "It is necessary to remind that by default the \"distance\" used in those cases is the Minkowski distance. With other parameters, it may have yielded other results, but for the purpose of this project it is a reasonable choice.\n",
    "\n",
    "These results yielded a comparable score, but it still looks like they get better as the number of neighbors considered increases.\n",
    "\n",
    "Let's retry the experiments with higher numbers of `n_neighbors`. This time, we will use the 'distance' value for the `weights` parameter, since it's clear that there are no significant differences between the different test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_neighbors += [9,10,11]\n",
    "for i in range(len(train_datasets)):\n",
    "    train = train_datasets[i]\n",
    "    test  = test_datasets[i]\n",
    "    for n in [9,10,11]:\n",
    "        print(f\"Using dataset {i} at {datetime.now()}: n_neighbor = {n}, weight = {w}\")\n",
    "        results.append(train_test_kNN(train[to_X], train[to_Y], test[to_X], test[to_Y], n))      \n",
    "        print(f\"Experiment number {i} ended succesfully at {datetime.now()}. \" + \n",
    "              f\"ETA: {(len(train_datasets) - i)*(len(n_neighbors) - (n - 8))} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results_kNN(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it looks like it slowly gets better as $k$ increases, the results still are very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVUf7WUQcI33"
   },
   "source": [
    "## Predictions - Attempt 2 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jkCHkx0Hdsj"
   },
   "source": [
    "### Why Linear Regression? - Introduction\n",
    "The idea behind the adoption of the LinReg model is correlated to the low integrity of the inititial dataset. \n",
    "\n",
    "In contrast with the previously analyzed model, this is an attempt to see what would happen with an \"assumption-free\" model. It is expected that this type of analysis will underline some unseen correlations, and also will produce some interesting predictions.\n",
    "\n",
    "This test will be conducted with the [ScikitLearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) of the LinearRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3N2szLiTv7Oi"
   },
   "outputs": [],
   "source": [
    "def train_test_LinReg(x_train, y_train, x_test, y_test):\n",
    "    model = LinearRegression().fit(x_train, y_train)\n",
    "    predictions = model.predict(x_test)\n",
    "    data = {\n",
    "        'predictions': predictions,\n",
    "        'R_sq': model.score(x_test, predictions),\n",
    "        'MSE': mean_squared_error(y_test, predictions),\n",
    "        'Adj_R_sq': 1 - (1-model.score(x_test, predictions))*(len(predictions)-1)/(len(predictions)-x_test.shape[1]-1)\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 13.9 s\n",
      "Wall time: 3.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = []\n",
    "to_Y = ['logerror']\n",
    "to_X = list(set(train.columns) - set('logerror'))\n",
    "for i in range(len(train_datasets)):\n",
    "    train = train_datasets[i]\n",
    "    test  = test_datasets[i]\n",
    "    for n in n_neighbors:\n",
    "        for w in types:\n",
    "            res.append(train_test_LinReg(train[to_X], train[to_Y], test[to_X], test[to_Y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHbgkxaTJqKL"
   },
   "source": [
    "### How did it go? - Evaluation\n",
    "We can now proceed with the evaluation of the results.\n",
    "\n",
    "The following tests will be used:\n",
    "\n",
    "\n",
    "*  Mean Squared Error\n",
    "*  R-squared index\n",
    "*  Adjusted R-squared index\n",
    "\n",
    "\n",
    "The evaluations will be done by using the [ScikitLearn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results_LinReg(results):\n",
    "    for i in range(len(results)):\n",
    "        parameters = results[i]\n",
    "        print(f\"Experiment number {i}: {' ' * (4 - floor(log10(i + 1)))} R_sq index = {parameters['R_sq']}, \" + \n",
    "              f\"MSE = {parameters['MSE']}, Adj_R_sq index = {parameters['Adj_R_sq']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment number 0:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 1:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 2:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 3:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 4:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 5:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 6:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 7:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 8:      R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 9:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 10:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 11:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 12:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 13:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 14:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 15:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 16:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 17:     R_sq index = 1.0, MSE = 8.56091832944962e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 18:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 19:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 20:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 21:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 22:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 23:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 24:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 25:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 26:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 27:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 28:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 29:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 30:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 31:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 32:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 33:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 34:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 35:     R_sq index = 1.0, MSE = 9.056467867475281e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 36:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 37:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 38:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 39:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 40:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 41:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 42:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 43:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 44:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 45:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 46:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 47:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 48:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 49:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 50:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 51:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 52:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 53:     R_sq index = 1.0, MSE = 9.229879966533565e-32, Adj_R_sq index = 1.0\n",
      "Experiment number 54:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 55:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 56:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 57:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 58:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 59:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 60:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 61:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 62:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 63:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 64:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 65:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 66:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 67:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 68:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 69:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 70:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n",
      "Experiment number 71:     R_sq index = 1.0, MSE = 1.3001279210990119e-31, Adj_R_sq index = 1.0\n"
     ]
    }
   ],
   "source": [
    "show_results_LinReg(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the results it is clear that the results are too good to be true. It is clear that this is a case of overfitting. [This article](https://statisticsbyjim.com/regression/r-squared-too-high/) explains the other possible reasons of these results.\n",
    "\n",
    "There are two main reasons for this kind of result:\n",
    "\n",
    "- `Wrong algorithm/approach`: the first and most probable cause, is the inner workings of the Linear Regression: it may just be the wrong model to obtain any useful information from.\n",
    "\n",
    "- `Biased evaluation indexes`: the second possibile cause is a bias contained in the testing measures. The analysis of other indexes may yield some more interesting results, but for now is impossible to declare the precise reason for the over-fitting to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the first regression algorithm is a more reasonable choice to do then the second one. \n",
    "\n",
    "The latter suffers from a clear case of over-fitting, so the results are totally biased.\n",
    "\n",
    "The k-NN algorithm didn't get the expected results, all the evaluation measures turned out to be very low.\n",
    "The causes may be found in the inadequacy of this algorithm to the context. It is advisable to use different algorithms and a more complete dataset to retrieve better results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Prediction - Simonato.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
