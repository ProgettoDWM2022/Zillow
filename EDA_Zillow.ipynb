{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brs_FcddrRXa"
   },
   "source": [
    "# Explorative Data Analysis\n",
    "In this notebook we will import the dataset from [Kaggle - Zillow Prize](https://www.kaggle.com/c/zillow-prize-1/data) and prepare it for prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEWIlH_L_RpC"
   },
   "source": [
    "## Problem definition\n",
    "The problem definition\n",
    "works as the driving force for a data analysis plan execution. \n",
    "\n",
    "How can we assess the value of a house, or better, the **sale price**? The short answer is we cannot, since the database is not made for that. However, we can predict the log-error.\n",
    "\n",
    "$logerror = \\log(Zestimate)-\\log(SalePrice)$\n",
    "\n",
    "Now, it is easier to understand what our real goal is: **predicting a number through regression**. This is something we know how to do!\n",
    "\n",
    "### Understand the context: hosing-market in the west coast of the U.S.A.\n",
    "Other than finding links between features, we need to point out some factors we cannot easily include when making our predictions.\n",
    "\n",
    "We are given a \"full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016 and 2017\", What problems are we facing now?\n",
    "1. We are focusing on a particular geographical area, which we are not familiar with;\n",
    "2. We are focusing on a market we have no prior domain-knowledge of;\n",
    "3. There is no price sale in the dataset.\n",
    "\n",
    "Therefore, we have not a clue of how the market is going for houses there, nor we know what gets a house to a particular sale price (the weight of each feature). While the first can be hardly tracked,\n",
    "we can find out about the latter two through data exploration and domain research.\n",
    "\n",
    "As we know, datasets rarely are perfect, so we must perform some *magic* to make it usable with our\n",
    "regression algorithms and find out useful information!\n",
    "\n",
    "\n",
    "### Need for a clean & clear dataset\n",
    "What do we need to do to achieve a **clean dataset**?\n",
    "\n",
    "The following approach allows us to use a vast variety of cleaning and feature engineering strategies to converge to a perfect training dataset.\n",
    "\n",
    "1. Basic Exploration\n",
    "2. Data Cleaning & Feature Engineering\n",
    "3. Modeling\n",
    "4. Export\n",
    "\n",
    "However, we will not follow a linear approach. We will improve the quality of the dataset as soon as we have the chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60PauvZq8QpC"
   },
   "source": [
    "## Step 1: Basic Exploration\n",
    "First of all, we need to import our data and set up our environment\n",
    "\n",
    "In this step, we define the sources of data, define data schemas\n",
    "and tables, understand the main characteristics of the data.\n",
    "\n",
    "The golden rule here is not to touch the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the dependencies (just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dependencies = ['pandas', 'numpy', 'missingno', 'seaborn', 'folium', 'sklearn', 'matplotlib', 'datetime','xgboost', 'lightgbm']\n",
    "for i in dependencies:\n",
    "    !pip install {i}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lzccW46E27Y"
   },
   "source": [
    "### Environment & Global Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1672,
     "status": "ok",
     "timestamp": 1652631927675,
     "user": {
      "displayName": "NiccolÃ² Simonato",
      "userId": "07293489282093960075"
     },
     "user_tz": -120
    },
    "id": "5CXTJrhxrVHe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "color = sns.color_palette()\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# It's important to set it up before anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1652631937137,
     "user": {
      "displayName": "NiccolÃ² Simonato",
      "userId": "07293489282093960075"
     },
     "user_tz": -120
    },
    "id": "iKruM5BovFRD"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hyLUY9OLxlF"
   },
   "source": [
    "### Data Loading\n",
    "Load the data, merge the years together and check out its shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2t2yuErGFwh"
   },
   "outputs": [],
   "source": [
    "# DataFrame downloaded and splitted into properties and train for 2016 and 2017 \n",
    "\n",
    "def load_merge_proptrain(dataset1, dataset2, predicate):\n",
    "    properties = pd.read_csv(dataset1, low_memory = False)\n",
    "    train = pd.read_csv(dataset2, low_memory = False)\n",
    "    # Let's merge the respective years \n",
    "    merged = pd.merge(properties, train, how = 'inner', on = predicate)\n",
    "    return merged\n",
    "\n",
    "\n",
    "data2016 = load_merge_proptrain(\"Data/properties_2016.csv\", \"Data/train_2016_v2.csv\", 'parcelid')\n",
    "data2017 = load_merge_proptrain(\"Data/properties_2017.csv\", \"Data/train_2017.csv\", 'parcelid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNliLSaXvFRF"
   },
   "source": [
    "Now, we have two choices\n",
    "* Merge them together\n",
    "    * Can complicate the learning process\n",
    "* Use the 2017 data to test our cleaning and feature engineering process\n",
    "    * We will still perform the same computations on both datasets\n",
    "    \n",
    "We keep them separated for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0eLqXhNvFRF",
    "outputId": "ebf5c148-34dc-43c9-c6dd-431b0f682782",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shape\n",
    "print(f'Shape of data2016 is:\\t {data2016.shape}')\n",
    "print(f'Shape of data2017 is:\\t {data2017.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CYo88kgvFRG"
   },
   "source": [
    "### Basic Info\n",
    "\n",
    "Let's find out some basic information about out dataset, such as:\n",
    "* Memory usage\n",
    "* Total count of entries (rows)\n",
    "* What features are present (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3BD-5SxId8F",
    "outputId": "a68cdcad-68a0-455f-9ee8-3224b22ccc66"
   },
   "outputs": [],
   "source": [
    "#Print avoiding Notebook Flood\n",
    "print('2016 House Dataset')\n",
    "print('============================================================')\n",
    "data2016.info(verbose=False)\n",
    "\n",
    "print('\\n2017 House Dataset')\n",
    "print('============================================================')\n",
    "data2017.info(verbose=False)\n",
    "\n",
    "print('\\n\\n2016 House Dataset Shape')\n",
    "print('============================================================')\n",
    "print(data2016.shape)\n",
    "print('\\n2017 House Dataset Shape')\n",
    "print('============================================================')\n",
    "print(data2017.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnshKli4G9mp",
    "outputId": "1268996e-b895-4660-e139-fd94827824f2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2016.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5H_khCSvFRH",
    "outputId": "e7da3b30-291d-4a67-92bd-b2a99923908c"
   },
   "outputs": [],
   "source": [
    "data2017.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydWwMreHvFRH"
   },
   "source": [
    "### Check data types & Study Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kc4pV79svFRH"
   },
   "source": [
    "We have found out that *float64(53), int64(1), object(6)*\n",
    "\n",
    "Let's analyze the 6 objects first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtGVVAQGvFRI",
    "outputId": "ebf405a7-76ec-4801-b35e-15dcd7f22955",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2016.select_dtypes(include = 'object').columns\n",
    "# data2017.select_dtypes(include = 'object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oztj3XwGvFRI"
   },
   "source": [
    "##### Categorical \n",
    "\n",
    "Quick fixes that we can take into account are:\n",
    "* Convert `transactiondate` into a format that is better fit\n",
    "* Encode `hashottuborspa`, `fireplaceflag`\n",
    "    * Add new feature `hashottub`, a number either 0 or 1\n",
    "    * Add new feature `hasspa`, a number either 0 or 1\n",
    "    * Add new feature `hasfireplace`, a number either 0 or 1\n",
    "    * remove `hashottuborspa`\n",
    "* Consider using the different labels of `propertyzoningdesc` for prediction\n",
    "    * Might fill the missing values with KNN\n",
    "* `taxdelinquencyflag`, `propertycountylandusecode`, `propertyzoningdesc` could be helpful but we still need to see their missing values rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFp3Bwp-vFRI"
   },
   "source": [
    "##### Numerical\n",
    "\n",
    "While for the numerical features, we can consider these options:\n",
    "* Convert all the float64 to float32, for faster computation.\n",
    "    * To avoid losing information we can rescale the data and then convert it all, so the change will be uniform\n",
    "* Convert int64 to float32, if it makes sense\n",
    "    * Remember we cannot say there is 1.5 pools in a house!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGsHzF7IvFRJ"
   },
   "source": [
    "#### Before taking any we study the correlation and the missing values rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxc_A9qmvFRJ"
   },
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDlOo4cmvFRJ",
    "outputId": "bcd19b02-14f4-4a9c-b72d-6e8b2212b94d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def numeric_heatmap(df, col_type):\n",
    "    plt.figure(figsize = (12,8))\n",
    "    sns.heatmap(data=df.select_dtypes(include=col_type).corr())\n",
    "    plt.show()\n",
    "    plt.gcf().clear()\n",
    "    pass\n",
    "\n",
    "cols = ['float64','int64'] #choose the type you want to see\n",
    "numeric_heatmap(data2016, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNfMOZr1vFRK"
   },
   "source": [
    "### Step 1.4:  Identify Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF1qoNK4vFRK"
   },
   "source": [
    "Here, we identify feature with more than 90% missing values.\n",
    "* If they are categorical, we need to carefully study them\n",
    "* If they are numerical, it feels more reasonable to drop them. Else they will be in the way of the estimation.\n",
    "\n",
    "Let's start by visualizing those features and then drop some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0PCMYqwvFRK",
    "outputId": "e2413a6b-a4ab-4293-a954-87ad9440c1a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def missing_values_barplot(dataset):\n",
    "    missing_df = dataset.isnull().sum(axis=0).reset_index()\n",
    "    missing_df.columns = ['column_name', 'missing_count']\n",
    "    missing_df = missing_df.loc[missing_df['missing_count']>0]\n",
    "    # missing_df = dataset.isnull().sum(axis = 0) * 100 / len(dataset)\n",
    "    missing_df = missing_df.sort_values(by='missing_count')\n",
    "\n",
    "    ind = np.arange(missing_df.shape[0])\n",
    "    width = 0.9\n",
    "    fig, ax = plt.subplots(figsize=(12,18))\n",
    "    rects = ax.barh(ind, missing_df.missing_count.values, color='green')\n",
    "    ax.set_yticks(ind)\n",
    "    ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "    ax.set_xlabel(\"Count of missing values\")\n",
    "    ax.set_title(\"Number of missing values in each column\")\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "missing_values_barplot(data2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDnzUrdBvFRK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#missing_values_barplot(data2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tei2enoLvFRK",
    "outputId": "d7c35b99-5529-496c-91dd-dba255c5e76e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_ratio = data2016.isna().sum().sort_values(ascending = False)/len(data2016)\n",
    "missing_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRr1Yo1SvFRL"
   },
   "source": [
    "We understand now that we have three kinds of problems here:\n",
    "* Missing more than 89%\n",
    "    * Probably best to drop, unless they are categorical\n",
    "* Missing more than 40%\n",
    "    * Could be time for some feature engineering \n",
    "    * Can be filled artificially\n",
    "* Missing 10% or less\n",
    "    * Can be filled artificially\n",
    "\n",
    "The first thing to is to drop some data, which might be in our way to predict the logerror. Too many feature become difficult to handle.\n",
    "\n",
    "##### Removing features that miss 89% of data or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3EsJkxDKvFRL",
    "outputId": "519788ae-4199-4aed-f6e1-5346d16e22a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_drop = missing_ratio[missing_ratio>=0.89].index.tolist()\n",
    "data2016[to_drop].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_ghWj-GvFRL"
   },
   "source": [
    "We will follow the strategy as stated before for every object present here, apart from `taxdelinquencyflag` which needs to be encoded.\n",
    "\n",
    "NA values in each variable means that there is no fireplace/hot tub(spa) and property taxes for this parcel are not past due as of 2015. That is, NA is **unique category** in those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6tMHQStvFRL"
   },
   "source": [
    "Just to be sure, let's plot a heat map to see if there is a pattern we can exploit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnJZD0cQvFRL",
    "outputId": "0fbbaaa2-1458-4930-9aad-4fcf2e0109d5"
   },
   "outputs": [],
   "source": [
    "def missing_values_heatmap(dataset):\n",
    "    missingValueColumns = dataset.columns[dataset.isnull().any()].tolist()\n",
    "    msno.heatmap(dataset[missingValueColumns],figsize=(30,30))\n",
    "    pass\n",
    "\n",
    "missing_values_heatmap(data2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eK8gRlnvvFRL"
   },
   "outputs": [],
   "source": [
    "# missing_values_heatmap(data2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7em-ugYcvFRL"
   },
   "source": [
    "We can see that some feature that are correlated, also are very redundant!\n",
    "\n",
    "By looking at the claims description they represent very similair pieces of information (the area of the property) lets pick the one with the fewest number of missing values and drop the rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07YhtFchJFNF"
   },
   "source": [
    "### Step 2.2: LogError Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc_L-7K5vFRM",
    "outputId": "c34f0408-f472-4a46-e860-9b8f4b2a5108"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(data2016.shape[0]), np.sort(data2016.logerror.values))\n",
    "plt.scatter(range(data2017.shape[0]), np.sort(data2017.logerror.values))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('logerror', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKX_OCJKvFRM",
    "outputId": "c2ba976f-9568-452b-e795-1368ffa390a9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.histplot(data2016.logerror.values, bins=100, kde=False)\n",
    "sns.histplot(data2017.logerror.values, bins=100, kde=False)\n",
    "plt.xlabel('logerror', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzWYvt4xvFRM"
   },
   "source": [
    "### Step 2.3: Transaction Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAq2OF6yvFRM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvMaskwGvFRM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyqlwqfhvFRM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_f2KUnevFRM"
   },
   "source": [
    "### Step 2.4: Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz46HPSovFRM"
   },
   "outputs": [],
   "source": [
    "numeric_heatmap(data2017, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyblIBhrvFRN"
   },
   "source": [
    "## Step 3: Cleaning and Feature Engineering\n",
    "\n",
    "We are going to create additional features to better direct our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufErcTn5vFRN"
   },
   "source": [
    "#### A feature problem\n",
    "\n",
    "Some of these columns are not useful for our purpose, this is why after an extensive research we understood the main driving factors for house prices:\n",
    "1. Neighborhood comps\n",
    "2. Location\n",
    "3. Home size and usable space\n",
    "4. Age and condition\n",
    "5. Upgrades and updates\n",
    "6. The local market and economic change\n",
    "8. Mortgage interest rate\n",
    "\n",
    "However, we do not have data for each one of those points and most of them are hard to represent in the way the datasets are given to us (even if wanted to add some features of our own).\n",
    "\n",
    "We have to face some difficult decisions now, which will mainly concern\n",
    "* Type Conversion\n",
    "* Rescaling\n",
    "* Dropping Values\n",
    "    * Outliers\n",
    "    * Empty Columns\n",
    "* Artificial Filling (if over 60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yqr5euFvFRN"
   },
   "source": [
    "### Step 3.1:  Additional features related to the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDjx7bcYvFRN"
   },
   "outputs": [],
   "source": [
    "def object_encoding(dataset1, dataset2):\n",
    "    properties = pd.read_csv(dataset1, low_memory = False)\n",
    "    train = pd.read_csv(dataset2, low_memory = False) \n",
    "    for c in properties.columns:\n",
    "        properties[c]=properties[c].fillna(-1)\n",
    "        if properties[c].dtype == 'object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(properties[c].values))\n",
    "            properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "    return train.merge(properties, how='inner', on='parcelid')\n",
    "\n",
    "\n",
    "df_train = object_encoding(\"Data/properties_2016.csv\", \"Data/train_2016_v2.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKk0MHGwvFRN"
   },
   "outputs": [],
   "source": [
    "#life of property\n",
    "df_train['N-life'] = 2022 - df_train['yearbuilt']\n",
    "\n",
    "#error in calculation of the finished living area of home\n",
    "df_train['N-LivingAreaError'] = df_train['calculatedfinishedsquarefeet']/df_train['finishedsquarefeet12']\n",
    "\n",
    "#proportion of living area\n",
    "df_train['N-LivingAreaProp'] = df_train['calculatedfinishedsquarefeet']/df_train['lotsizesquarefeet']\n",
    "df_train['N-LivingAreaProp2'] = df_train['finishedsquarefeet12']/df_train['finishedsquarefeet15']\n",
    "\n",
    "#Amout of extra space\n",
    "df_train['N-ExtraSpace'] = df_train['lotsizesquarefeet'] - df_train['calculatedfinishedsquarefeet'] \n",
    "df_train['N-ExtraSpace-2'] = df_train['finishedsquarefeet15'] - df_train['finishedsquarefeet12'] \n",
    "\n",
    "#Total number of rooms\n",
    "df_train['N-TotalRooms'] = df_train['bathroomcnt']*df_train['bedroomcnt']\n",
    "\n",
    "#Average room size\n",
    "df_train['N-AvRoomSize'] = df_train['calculatedfinishedsquarefeet']/df_train['roomcnt'] \n",
    "\n",
    "# Number of Extra rooms\n",
    "df_train['N-ExtraRooms'] = df_train['roomcnt'] - df_train['N-TotalRooms'] \n",
    "\n",
    "#Ratio of the built structure value to land area\n",
    "df_train['N-ValueProp'] = df_train['structuretaxvaluedollarcnt']/df_train['landtaxvaluedollarcnt']\n",
    "\n",
    "#Does property have a garage, pool or hot tub and AC?\n",
    "df_train['N-GarPoolAC'] = ((df_train['garagecarcnt']>0) & (df_train['pooltypeid10']>0) & (df_train['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "df_train[\"N-location\"] = df_train[\"latitude\"] + df_train[\"longitude\"]\n",
    "df_train[\"N-location-2\"] = df_train[\"latitude\"]*df_train[\"longitude\"]\n",
    "df_train[\"N-location-2round\"] = df_train[\"N-location-2\"].round(-4)\n",
    "\n",
    "df_train[\"N-latitude-round\"] = df_train[\"latitude\"].round(-4)\n",
    "df_train[\"N-longitude-round\"] = df_train[\"longitude\"].round(-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7Pyb1cOvFRN"
   },
   "source": [
    "#### Step 3.1.2:  Additional features based off the tax related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ia0AMWX3vFRN"
   },
   "outputs": [],
   "source": [
    "#Ratio of tax of property over parcel\n",
    "df_train['N-ValueRatio'] = df_train['taxvaluedollarcnt']/df_train['taxamount']\n",
    "\n",
    "#TotalTaxScore\n",
    "df_train['N-TaxScore'] = df_train['taxvaluedollarcnt']*df_train['taxamount']\n",
    "\n",
    "#polnomials of tax delinquency year\n",
    "df_train[\"N-taxdelinquencyyear-2\"] = df_train[\"taxdelinquencyyear\"] ** 2\n",
    "df_train[\"N-taxdelinquencyyear-3\"] = df_train[\"taxdelinquencyyear\"] ** 3\n",
    "\n",
    "#Length of time since unpaid taxes\n",
    "df_train['N-life'] = 2018 - df_train['taxdelinquencyyear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OZ8b4f3vFRO"
   },
   "source": [
    "#### Step 3.1.3: Other features based off the location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4rKDfSWvFRO"
   },
   "outputs": [],
   "source": [
    "#Number of properties in the zip\n",
    "zip_count = df_train['regionidzip'].value_counts().to_dict()\n",
    "df_train['N-zip_count'] = df_train['regionidzip'].map(zip_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "city_count = df_train['regionidcity'].value_counts().to_dict()\n",
    "df_train['N-city_count'] = df_train['regionidcity'].map(city_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "region_count = df_train['regionidcounty'].value_counts().to_dict()\n",
    "df_train['N-county_count'] = df_train['regionidcounty'].map(city_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIwlsnwgvFRO"
   },
   "source": [
    "#### Step 3.1.4: Additional variables which are simplification of some of the other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCll38ZHvFRO"
   },
   "outputs": [],
   "source": [
    "#Indicator whether it has AC or not\n",
    "df_train['N-ACInd'] = (df_train['airconditioningtypeid']!=5)*1\n",
    "\n",
    "#Indicator whether it has Heating or not \n",
    "df_train['N-HeatInd'] = (df_train['heatingorsystemtypeid']!=13)*1\n",
    "\n",
    "#There's 25 different property uses - let's compress them down to 4 categories\n",
    "df_train['N-PropType'] = df_train.propertylandusetypeid.replace({31 : \"Mixed\", 46 : \"Other\", 47 : \"Mixed\", 246 : \"Mixed\", 247 : \"Mixed\", 248 : \"Mixed\", 260 : \"Home\", 261 : \"Home\", 262 : \"Home\", 263 : \"Home\", 264 : \"Home\", 265 : \"Home\", 266 : \"Home\", 267 : \"Home\", 268 : \"Home\", 269 : \"Not Built\", 270 : \"Home\", 271 : \"Home\", 273 : \"Home\", 274 : \"Other\", 275 : \"Home\", 276 : \"Home\", 279 : \"Home\", 290 : \"Not Built\", 291 : \"Not Built\" })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFteVggSvFRO"
   },
   "source": [
    "#### Step 3.1.5: Extra\n",
    "One of the EDA kernels indicated that structuretaxvaluedollarcnt was one of the most important features. \n",
    "So let's create some additional variables on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1lzzGwavFRO"
   },
   "outputs": [],
   "source": [
    "#polnomials of the variable\n",
    "df_train[\"N-structuretaxvaluedollarcnt-2\"] = df_train[\"structuretaxvaluedollarcnt\"] ** 2\n",
    "df_train[\"N-structuretaxvaluedollarcnt-3\"] = df_train[\"structuretaxvaluedollarcnt\"] ** 3\n",
    "\n",
    "#Average structuretaxvaluedollarcnt by city\n",
    "group = df_train.groupby('regionidcity')['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\n",
    "df_train['N-Avg-structuretaxvaluedollarcnt'] = df_train['regionidcity'].map(group)\n",
    "\n",
    "#Deviation away from average\n",
    "df_train['N-Dev-structuretaxvaluedollarcnt'] = abs((df_train['structuretaxvaluedollarcnt'] - df_train['N-Avg-structuretaxvaluedollarcnt']))/df_train['N-Avg-structuretaxvaluedollarcnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m4rXX8wJai4"
   },
   "source": [
    "### Step 3.2: Data Cleaning\n",
    "We need to identify\n",
    "- Missing data\n",
    "- Unhelpful features\n",
    "- Non-numeric data\n",
    "- Data that needs to be rescaled \n",
    "\n",
    "We need to plan\n",
    "- Featuring engineering\n",
    "- How to adapt/transform the data\n",
    "\n",
    "---\n",
    "\n",
    "To deal with Missing Data:\n",
    "\n",
    "1st Method: Drop Missing Data\n",
    "- a. Drop the whole row OR \n",
    "- b. Drop the whole column (This should be used ONLY if most entries are empty in your column)\n",
    "\n",
    "2nd Method: Replace data \n",
    "- a. Replace it by mean OR\n",
    "- b. Replace it by frequency (mode) OR \n",
    "- c. Replace it based on other functions\n",
    "\n",
    "---\n",
    "\n",
    "In section 1.4 we identified the columns that were missing more than 89% of data. We can start by dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed the less significant part of the dataset, we can now proceed to fill the columns' cells whose missing data ratio is over 60%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2.1 Features with missing data percentage above 60% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fill = missing_ratio[missing_ratio>=0.6].index.tolist() # list of columns to fill\n",
    "to_fill = list(set(to_fill) - set(to_drop)) # we don't need to consider the dropped columns\n",
    "\n",
    "for col in to_fill:\n",
    "    print(f\"[{col}] {' ' * (23 - len(col) - 2)}-> {missing_ratio[col]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features need to be filled in some way. Let's start with some quick fixes:\n",
    "- 'poolcnt' is the number of pools located in the considered house. It's pretty safe to assume that if the data is missing, probably there are none, so we could just fill these cells with a 0.\n",
    "\n",
    "- 'pooltypeid7', similarly as the previous feature, we can assume that the missing value means that there's no pool. In fact, the missing value percentage is similar to the previous one. In this case, this feature signals if the pool has an hot tub or not. We will fill the cells with the value -1, that means \"no hot tub\".\n",
    "\n",
    "- 'garagecarcnt' is the number of garages that are present on the lot. As we previously did, we are going to assume that a missing value means that there are none. The empty cells are going to be filled with 0.\n",
    "\n",
    "- 'garagetotalsqft' represents the surface occupied by the garages. The missing values percentage is the same as 'garagecarcnt', so we are going to fill the empty cells with 0, because we are assuming that that's the meaning of a missing value.\n",
    "\n",
    "- 'airconditioningtypeid' presents the same situation we have discussed for the previous features. In this case, we are going to fill the empty cells with the value 5, that represents the absence of an air-conditioning system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['poolcnt'] = df_train['poolcnt'].fillna(0) #filling the empty cells of 'poolcnt'\n",
    "df_train['pooltypeid7'] = df_train['pooltypeid7'].fillna(-1) #filling the empty cells of 'pooltypeid7'\n",
    "df_train['garagecarcnt'] = df_train['garagecarcnt'].fillna(0) #filling the empty cells of 'garagecarcnt'\n",
    "df_train['garagetotalsqft'] = df_train['garagetotalsqft'].fillna(0) #filling the empty cells of 'garagetotalsqft'\n",
    "df_train['airconditioningtypeid'] = df_train['airconditioningtypeid'].fillna(5) #filling the empty cells of ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining features need some considerations before doing any operation on them.\n",
    "- 'regionidneighborhood' does not represent a property of the house itself, but an id assigned by the local administration. It will be reasonable to fill these missing values with the mean of the values of the literal neighbors. An implementation of the k-NN algorithm may come at hand. The k parameter can be set as 10.\n",
    "\n",
    "- 'numberofstories' reprents the number of stories of the house. Usually, the houses of a neighborhood do not differ significantly from each others. As we saw for the previuous feature, it may be useful to use the k-NN algorithm to fill the empty cells of this column, basing on the geografic location and the architechtural style of the house. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.loc[ == False].shape\n",
    "df_train['regionidneighborhood'].bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'regionidneighborhood' data filling\n",
    "#selecting the training features that we need\n",
    "train_x = df_train[['latitude','longitude']]\n",
    "train_y = df_train['regionidneighborhood']\n",
    "#selecting the rows that we need to fill\n",
    "df_train.loc(df_train['regionidneighborhood'].isnull() == True)\n",
    "neigh_regionidneighborhood = KNeighborsClassifier(n_neighbors=10)\n",
    "neigh_regionidneighborhood.fit(train_x, train_y)\n",
    "neigh_regionidneighborhood.predict()\n",
    "#filling the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'numberofstories' data filling\n",
    "#selecting the training features that we need\n",
    "train_x = df_train[['latitude','longitude','architecturalstyletypeid']]\n",
    "train_y = df_train['numberofstories']\n",
    "#selecting the rows that we need to fill\n",
    "neigh_numberofstories = KNeighborsRegressor(n_neighbors=2)\n",
    "neigh_numberofstories.fit(train_x, train_y)\n",
    "neigh_numberofstories.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2.2 Features with missing data percentage below 60% \n",
    "We will now proceed with the managing the other features. We will now consider the ones whose missing data percentage is below 60%. Due to time-economy reasons, we will consider just the ones whose percentage is above 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fill_60p = missing_ratio[missing_ratio<0.6].index.tolist() # list of columns to fill, below 60%\n",
    "to_fill_5p = missing_ratio[missing_ratio>=0.05].index.tolist() # list of columns to fill, above 5%\n",
    "\n",
    "to_fill_5p = list(set(to_fill_5p) - set(to_drop) - set(to_fill)) # we don't need to consider the other columns\n",
    "\n",
    "\n",
    "for col in to_fill_5p:\n",
    "    print(f\"[{col}] {' ' * (30 - len(col) - 2)}-> {missing_ratio[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlbnQCbHvFRP"
   },
   "source": [
    "### Step 3.3 DataType conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlgJJOJ9vFRP"
   },
   "outputs": [],
   "source": [
    "#### Step 3.3.1 Sales date conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQpIXqF6vFRP"
   },
   "outputs": [],
   "source": [
    "#### Step 3.3.2 DataType "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fCCx_OlTyYk"
   },
   "source": [
    "## Step 4: Development and Modeling\n",
    "This step involves presenting\n",
    "the dataset to the target audience in the form of graphs, summary tables, maps,\n",
    "and diagrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmaSD9VwvFRO"
   },
   "source": [
    "#### Step 4.1: Asserting feature importance\n",
    "Lets use XGBoost to assess importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5CYRJMFvFRO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_y = df_train['logerror'].values\n",
    "df_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n",
    "feat_names = df_train.columns.values\n",
    "\n",
    "for c in df_train.columns:\n",
    "    if df_train[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df_train[c].values))\n",
    "        df_train[c] = lbl.transform(list(df_train[c].values))\n",
    "\n",
    "#import xgboost as xgb\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'silent': 1,\n",
    "    'seed' : 0\n",
    "}\n",
    "dtrain = xgb.DMatrix(df_train, train_y, feature_names=df_train.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=150)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqtWjEhGvFRP"
   },
   "outputs": [],
   "source": [
    "featureImportance = model.get_fscore()\n",
    "features = pd.DataFrame()\n",
    "features['features'] = featureImportance.keys()\n",
    "features['importance'] = featureImportance.values()\n",
    "features.sort_values(by=['importance'],ascending=False,inplace=True)\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(data=features.head(15),x=\"importance\",y=\"features\",ax=ax,orient=\"h\",color=\"#34495e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u6OMMCRvFRP"
   },
   "source": [
    "### Step 4.2: Focus on logerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ykAyo7eQvFRP"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mergedFilterd = merged.fillna(-999)\n",
    "for f in mergedFilterd.columns:\n",
    "    if mergedFilterd[f].dtype=='object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(mergedFilterd[f].values)) \n",
    "        mergedFilterd[f] = lbl.transform(list(mergedFilterd[f].values))\n",
    "        \n",
    "train_y = mergedFilterd.logerror.values\n",
    "train_X = mergedFilterd.drop([\"parcelid\", \"transactiondate\", \"logerror\"], axis=1)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "dtrain = xgb.DMatrix(train_X, train_y, feature_names=train_X.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvCiI4fjvFRQ"
   },
   "outputs": [],
   "source": [
    "featureImportance = model.get_fscore()\n",
    "features = pd.DataFrame()\n",
    "features['features'] = featureImportance.keys()\n",
    "features['importance'] = featureImportance.values()\n",
    "features.sort_values(by=['importance'],ascending=False,inplace=True)\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "plt.xticks(rotation=90)\n",
    "sn.barplot(data=features.head(15),x=\"importance\",y=\"features\",ax=ax,orient=\"h\",color=\"#34495e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aKCxo5vvFRQ"
   },
   "outputs": [],
   "source": [
    "### 4.2.1: TP Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTT4CfUIvFRQ"
   },
   "outputs": [],
   "source": [
    "topFeatures = features[\"features\"].tolist()[:20]\n",
    "corrMatt = merged[topFeatures].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "sn.heatmap(corrMatt, mask=mask,vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T74qQmQsvFRQ"
   },
   "source": [
    "### 4.3: Multicollineraity analysis\n",
    "In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation, the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdzjCG2SvFRQ"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def calculate_vif_(X):\n",
    "    variables = list(X.columns)\n",
    "    vif = {variable:variance_inflation_factor(exog=X.values, exog_idx=ix) for ix,variable in enumerate(list(X.columns))}\n",
    "    return vif\n",
    "\n",
    "\n",
    "numericalCol = []\n",
    "for f in merged.columns:\n",
    "    #print (f)\n",
    "    if merged[f].dtype!='object' and f not in [\"parcelid\", \"transactiondate\", \"logerror\"]:\n",
    "        numericalCol.append(f)\n",
    "mergedFilterd = merged[numericalCol].fillna(-999)\n",
    "vifDict = calculate_vif_(mergedFilterd)\n",
    "\n",
    "vifDf = pd.DataFrame()\n",
    "vifDf['variables'] = vifDict.keys()\n",
    "vifDf['vifScore'] = vifDict.values()\n",
    "vifDf.sort_values(by=['vifScore'],ascending=False,inplace=True)\n",
    "validVariables = vifDf[vifDf[\"vifScore\"]<=5]\n",
    "variablesWithMC  = vifDf[vifDf[\"vifScore\"]>5]\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(ncols=2)\n",
    "fig.set_size_inches(20,8)\n",
    "sn.barplot(data=validVariables,x=\"vifScore\",y=\"variables\",ax=ax1,orient=\"h\",color=\"#34495e\")\n",
    "sn.barplot(data=variablesWithMC.head(5),x=\"vifScore\",y=\"variables\",ax=ax2,orient=\"h\",color=\"#34495e\")\n",
    "ax1.set(xlabel='VIF Scores', ylabel='Features',title=\"Valid Variables Without Multicollinearity\")\n",
    "ax2.set(xlabel='VIF Scores', ylabel='Features',title=\"Variables Which Exhibit Multicollinearity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HE9ULoHcvFRQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPGVvSmnvFRQ"
   },
   "source": [
    "### Step 4.4 Top Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQzY7wgEvFRQ"
   },
   "outputs": [],
   "source": [
    "# Target variable for this competition is \"logerror\" field. \n",
    "# So let us do some analysis on this field first.\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(range(data2016.shape[0]), np.sort(data2016.logerror.values))\n",
    "plt.scatter(range(data2017.shape[0]), np.sort(data2017.logerror.values))\n",
    "plt.xlabel('index', fontsize=12)\n",
    "plt.ylabel('logerror', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RfPPd1gvFRQ"
   },
   "outputs": [],
   "source": [
    "ulimit = np.percentile(merged.logerror.values, 99)\n",
    "llimit = np.percentile(merged.logerror.values, 1)\n",
    "merged['logerror'].ix[merged['logerror']>ulimit] = ulimit\n",
    "merged['logerror'].ix[merged['logerror']<llimit] = llimit\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "fig.set_size_inches(20,5)\n",
    "sn.distplot(merged.logerror.values, bins=50,kde=False,color=\"#34495e\",ax=ax)\n",
    "ax.set(xlabel='logerror', ylabel='VIF Score',title=\"Distribution Of Dependent Variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s40sY7o7vFRQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4E5GZQXvFRR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XeYrw28vFRR"
   },
   "source": [
    "## Step 5: Cross Validation & Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdAgaYMBxnvp"
   },
   "source": [
    "### Cross Validation\n",
    "The threat of overfitting is always behind the corner. \n",
    "\n",
    "Overfitting is the low predictive power of the model, caused by the use of the same dataset for the training phase and the testing phase. Since the goal of the model is to \"adapt\" to the data on which it trains on, testing its accuracy on the same dataset would lead to some unaccurate results.\n",
    "\n",
    "For this reason, we adopted the cross validation, which is a technique that prevents this phenomenon. \n",
    "\n",
    "We chose to split the dataset using the k-fold algorithm, by using the library provided by [Scikitlearn](https://https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "This method consist in splitting the dataset in k groups (the \"folds\") and keeping one out as the testing set. This procedure is repeated for each one of the folds, in order to minimize the chances of overfitting.\n",
    "\n",
    "Since we dispose of two main datasets, one for the year 2016 and one for the year 2017, we will apply the k-fold algorithm on both of them, and cross the resulting folds to maximize the randomness of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lw7ez8PavFRR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "df_train_folded16, df_test_folded16 = [], []\n",
    "df_train_folded17, df_test_folded17 = [], []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(df_train16):\n",
    "  df_train_folded16.append(df_train16[train_index])\n",
    "  df_test_folded16.append(df_train16[train_test])\n",
    "\n",
    "for train_index, test_index in kf.split(df_train17):\n",
    "  df_train_folded17.append(df_train17[train_index])\n",
    "  df_test_folded17.append(df_train17[train_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq1yZTgixsU-"
   },
   "source": [
    "### Exporting the cleaned dataset\n",
    "\n",
    "The following snippet will export the cleaned dataset in a series of file containing the testing and training dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkE-g-Iv3_AY"
   },
   "outputs": [],
   "source": [
    "#2016 dataset\n",
    "i = 0\n",
    "for ds in df_train_folded16:\n",
    "  i += 1\n",
    "  ds.to_csv(f\"Data/train_dataset_2016_{i}\")\n",
    "\n",
    "i = 0\n",
    "for ds in df_test_folded16:\n",
    "  i += 1\n",
    "  ds.to_csv(f\"Data/test_dataset_2016_{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-3REtam4xOX"
   },
   "outputs": [],
   "source": [
    "#2017 dataset\n",
    "i = 0\n",
    "for ds in df_train_folded17:\n",
    "  i += 1\n",
    "  ds.to_csv(f\"Data/train_dataset_2017_{i}\")\n",
    "\n",
    "i = 0\n",
    "for ds in df_test_folded17:\n",
    "  i += 1\n",
    "  ds.to_csv(f\"Data/test_dataset_2017_{i}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Cl7HqQIvFRR"
   },
   "source": [
    "##### Credits\n",
    "Credits go to: \n",
    "[Simple Exploration Notebook](https://www.kaggle.com/code/sudalairajkumar/simple-exploration-notebook-zillow-prize)\n",
    "[Zillow analysis with EDAðŸ ](https://www.kaggle.com/code/hyewon328/zillow-analysis-with-eda)\n",
    "[Zillow EDA On Missing Values & Multicollinearity](https://www.kaggle.com/code/viveksrinivasan/zillow-eda-on-missing-values-multicollinearity)\n",
    "[Creating Additional Features](https://www.kaggle.com/code/nikunjm88/creating-additional-features)\n",
    "[Simple EDA Geo Data & Time Series](https://www.kaggle.com/code/kueipo/simple-eda-geo-data-time-series)\n",
    "[Carefully dealing with missing values](https://www.kaggle.com/code/nikunjm88/carefully-dealing-with-missing-values)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EDA_Zillow.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
